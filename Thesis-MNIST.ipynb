{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification of MNIST dataset \n",
    "using Gen\n",
    "using PyPlot\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using Distances\n",
    "using Flux\n",
    "using StatsBase\n",
    "using MultivariateStats\n",
    "using MLDatasets\n",
    "using IterTools #Only for plots, not for program\n",
    "\n",
    "#include(\"hmc_mod.jl\")\n",
    "#include(\"helper_functions.jl\")\n",
    "include(\"proposals.jl\")\n",
    "include(\"mnist.jl\")\n",
    "include(\"utils.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the MNIST Data\n",
    "num_samples = 10\n",
    "x_train, y_train = load_mnist_train_set()\n",
    "x_test, y_test = load_mnist_test_set()\n",
    "\n",
    "x, y = balanced_set(x_train,y_train,num_samples);\n",
    "\n",
    "#Test Set\n",
    "x_test, y_test = balanced_set(x_test,y_test,num_samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Samples\n",
    "x_reshaped = reshape_x(x)\n",
    "\n",
    "nrow, ncol = 10,num_samples \n",
    "fig = figure(\"plot_mnist\",figsize=(6,6))\n",
    "for (i, (c, r)) in enumerate(Iterators.product(1:ncol, 1:nrow))\n",
    "    subplot(nrow, ncol, i)\n",
    "    imshow(x_reshaped[:,:,i]', cmap=\"gray\")\n",
    "    ax = gca()\n",
    "    ax.xaxis.set_visible(false)\n",
    "    ax.yaxis.set_visible(false)\n",
    "end;\n",
    "tight_layout(w_pad=-1, h_pad=-1, pad=-0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run PCA to lower dimensionality of MNIST\n",
    "vars = []\n",
    "x = transpose(x)\n",
    "for i=1:100    \n",
    "    MNIST_PCA = fit(PCA, x, maxoutdim=i, pratio=1.0)\n",
    "    push!(vars,MNIST_PCA.tprinvar/MNIST_PCA.tvar)\n",
    "end\n",
    "plot(vars)\n",
    "plt.title(\"PCA for MNIST Dataset\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Ratio of Explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get PCA Transform for x\n",
    "dims = 20\n",
    "x_pca = fit(PCA,x,maxoutdim=dims)\n",
    "xt = transform(x_pca,x)\n",
    "\n",
    "#Testing PCA Transform\n",
    "xz = transform(x_pca,transpose(x_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encode Y\n",
    "yt = Flux.onehotbatch(y,[:1,:2,:3,:4,:5,:6,:7,:8,:9,:10]);\n",
    "\n",
    "#Test Set\n",
    "yz = y_test\n",
    "yzt = Flux.onehotbatch(yz,[:1,:2,:3,:4,:5,:6,:7,:8,:9,:10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "#Hyperparameters and Helper Functions\n",
    "#------------------------------------\n",
    "\n",
    "#Select Network Goal\n",
    "network = \"classifier\"\n",
    "\n",
    "#Data hyperparameters\n",
    "n = num_samples #Number of samples\n",
    "c = 10 #Number of classes\n",
    "d = dims #Input dimension\n",
    "N = n*c #Total samples\n",
    "\n",
    "#Network hyperparameters\n",
    "α₁ = 1 #Input Weights, Biases Shape\n",
    "β₁ = 1 #Input Weights, Biases Scale/Rate\n",
    "α₂ = 1 #Hidden & Output Weights Shape\n",
    "\n",
    "#Node hyperparameters\n",
    "k_range = 10 #Maximum number of neurons per layer\n",
    "k_list = [Int(i) for i in 1:k_range]\n",
    "\n",
    "#NUTS\n",
    "Δ_max = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Softmax\n",
    "function softmax_(arr::AbstractArray)\n",
    "    ex = mapslices(x -> exp.(x),arr,dims=1)\n",
    "    rows, cols = size(arr)\n",
    "    val = similar(ex)\n",
    "    for i in 1:cols\n",
    "        s = sum(ex[:,i])\n",
    "        for j in 1:rows\n",
    "            val[j,i] = ex[j,i]/s\n",
    "        end\n",
    "    end\n",
    "    return val\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Neural Net\n",
    "function G(x, trace)\n",
    "    activation = relu\n",
    "    layers = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:layers]\n",
    "    \n",
    "    for i=1:layers\n",
    "        in_dim, out_dim = layer_unpacker(i, layers, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,layers+1)], c, ks[layers])\n",
    "    bₒ = reshape(trace[(:b,layers+1)], c)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    x = nn_out(x)\n",
    "    \n",
    "    return softmax_(x)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function classifier(x::Array{Float64})\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers - 1 for Classifier Net\n",
    "    l ~ categorical([1.0])\n",
    "    l_real = l\n",
    "    obs[:l] = l\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    for i=1:l\n",
    "        k[i] = @trace(categorical([1/length(k_list) for i=1:length(k_list)]), (:k,i))\n",
    "        obs[(:k,i)] = k[i]\n",
    "    end\n",
    "    output_array = zeros(Float64, 10)\n",
    "    output_array[10] = 1.0\n",
    "\n",
    "    k[l+1] = @trace(categorical(output_array), (:k,l+1))\n",
    "    obs[(:k,l+1)] = k[l+1]\n",
    "    \n",
    "    ##################################################\n",
    "    #Adapted from BH: hyperparameter schedule - Apr 3#\n",
    "    ##################################################\n",
    "    \n",
    "    α₁ = 1 #Input Weights, Biases Shape\n",
    "    β₁ = 1 #Input Weights, Biases Scale/Rate\n",
    "    α₂ = 1 #Hidden & Output Weights Shape\n",
    "    β₂ = k[1] #Scale Hyperparameter Based on Number of Nodes\n",
    "    \n",
    "    τ = [0.0 for i=1:l+1]\n",
    "    τᵦ = [0.0 for i=1:l+1]\n",
    "    σ = [0.0 for i=1:l+1]\n",
    "    σᵦ = [0.0 for i=1:l+1]\n",
    "    \n",
    "    for i=1:l+1\n",
    "        if i==1\n",
    "            τ[i] = @trace(gamma(α₁,β₁), (:τ,i))\n",
    "            τᵦ[i] = @trace(gamma(α₁,β₁), (:τᵦ,i))\n",
    "        else\n",
    "            τ[i] = @trace(gamma(α₂,β₂), (:τ,i))\n",
    "            τᵦ[i] = @trace(gamma(α₁,β₁), (:τᵦ,i))\n",
    "        end\n",
    "        σ[i] = 1/τ[i]\n",
    "        σᵦ[i] = 1/τᵦ[i]\n",
    "    end\n",
    "    \n",
    "    #Sample weight and bias vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "\n",
    "    for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            u = zeros(h)\n",
    "            S = Diagonal([σ[i] for j=1:length(u)])\n",
    "            W[i] = @trace(mvnormal(u,S), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            ub = zeros(k[i])\n",
    "            Sb = Diagonal([σᵦ[i] for j=1:length(ub)])   \n",
    "            b[i] = @trace(mvnormal(ub,Sb), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            u = zeros(h)\n",
    "            S = Diagonal([σ[i] for j=1:length(u)])\n",
    "            W[i] = @trace(mvnormal(u,S), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            ub = zeros(10)\n",
    "            Sb = Diagonal([σᵦ[i] for j=1:length(ub)]) \n",
    "            b[i] = @trace(mvnormal(ub,Sb), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = G(x,obs)\n",
    "    \n",
    "    #Logistic Classification Likelihood\n",
    "    y = zeros(length(scores))\n",
    "    for j=1:length(x[1,:])\n",
    "        score_vec = scores[:,j]\n",
    "        #println(score_vec)\n",
    "        y[j] = @trace(categorical(score_vec), (:y,j))\n",
    "    end\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "end;\n",
    "\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(y)\n",
    "    obs_master[(:y,i)] = y[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "\n",
    "(best_trace,) = generate(classifier, (xt,), obs)\n",
    "println(get_score(best_trace))\n",
    "println(best_trace[(:k,1)])\n",
    "test_labels = G(xt,best_trace)\n",
    "results = label_output(test_labels)\n",
    "test_acc = sum([y[i] == results[i] for i=1:length(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"proposals.jl\")\n",
    "(best_trace,) = generate(classifier, (xt,), obs)\n",
    "new_trace,q = node_birth(best_trace)\n",
    "display(reshape(best_trace[(:W,1)], best_trace[(:k,1)], d))\n",
    "display(reshape(new_trace[(:W,1)], new_trace[(:k,1)], d))\n",
    "println(get_score(best_trace))\n",
    "println(get_score(new_trace))\n",
    "\n",
    "test_labels = G(xt,best_trace)\n",
    "results = label_output(test_labels)\n",
    "test_acc = sum([y[i] == results[i] for i=1:length(y)])\n",
    "println(\"Old trace Accuracy: $test_acc\")\n",
    "\n",
    "test_labels = G(xt,new_trace)\n",
    "results = label_output(test_labels)\n",
    "test_acc = sum([y[i] == results[i] for i=1:length(y)])\n",
    "println(\"New trace Accuracy: $test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"proposals.jl\")\n",
    "(best_trace,) = generate(classifier, (xt,), obs)\n",
    "println(\"Current k: $(best_trace[(:k,1)])\")\n",
    "new_trace,q = node_death(best_trace)\n",
    "display(reshape(best_trace[(:W,1)], best_trace[(:k,1)], d))\n",
    "display(reshape(new_trace[(:W,1)], new_trace[(:k,1)], d))\n",
    "println(get_score(best_trace))\n",
    "println(get_score(new_trace))\n",
    "\n",
    "test_labels = G(xt,best_trace)\n",
    "results = label_output(test_labels)\n",
    "test_acc = sum([y[i] == results[i] for i=1:length(y)])\n",
    "println(\"Old trace Accuracy: $test_acc\")\n",
    "\n",
    "test_labels = G(xt,new_trace)\n",
    "results = label_output(test_labels)\n",
    "test_acc = sum([y[i] == results[i] for i=1:length(y)])\n",
    "println(\"New trace Accuracy: $test_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Likelihood\n",
    "obs[(:k,1)] = 10\n",
    "scores = []\n",
    "accs = []\n",
    "for i = 1:1000\n",
    "    (trace,) = generate(classifier, (xt,), obs)\n",
    "    push!(scores,get_score(trace))\n",
    "    test_labels = G(xt,trace)\n",
    "    results = label_output(test_labels)\n",
    "    test_acc = sum([y[i] == results[i] for i=1:length(y)])/length(y)\n",
    "    test_acc = sum([euclidean(yt[:,j],test_labels[:,j]) for j=1:length(y)])\n",
    "    push!(accs,test_acc)\n",
    "end\n",
    "\n",
    "scatter(accs,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"NUTS.jl\")\n",
    "(best_trace,) = generate(classifier, (xt,), obs)\n",
    "l = best_trace[:l]\n",
    "param_selection = select()\n",
    "for i=1:l+1 #Number of Layers\n",
    "    push!(param_selection, (:W,i))\n",
    "    push!(param_selection, (:b,i))\n",
    "end\n",
    "traces = NUTS(best_trace, param_selection, 0.65, 100, 3, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Straight NUTS\n",
    "include(\"NUTS.jl\")\n",
    "Δ_max = 10000\n",
    "\n",
    "@gen function gibbs_hyperparameters(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    for i=1:trace[:l] + 1\n",
    "        #Biases\n",
    "        bias = trace[(:b,i)]\n",
    "        \n",
    "        n = length(bias)\n",
    "        α = α₁ + (n/2)\n",
    "        \n",
    "        Σ = sum(bias.^2)/2 \n",
    "        β = 1/(1/β₁ + Σ)\n",
    "        \n",
    "        τᵦ ~ gamma(α,β)\n",
    "        \n",
    "        #Weights\n",
    "        β₂ = trace[(:k,1)]\n",
    "        \n",
    "        i == 1 ? α₀ = α₁ : α₀ = α₂\n",
    "        i == 1 ? β₀ = β₁ : β₀ = β₂\n",
    "        \n",
    "        weight = trace[(:W,i)]\n",
    "        \n",
    "        n = length(weight)\n",
    "        α = α₀ + (n/2)\n",
    "        \n",
    "        Σ = sum(weight.^2)/2\n",
    "        β = 1/(1/β₀ + Σ)\n",
    "        \n",
    "        τ ~ gamma(α,β)\n",
    "        \n",
    "        obs_new[(:τ,i)] = τ\n",
    "        obs_new[(:τᵦ,i)] = τᵦ\n",
    "    end\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "m=3\n",
    "iters=1000\n",
    "\n",
    "(best_trace,) = generate(classifier, (xt,), obs)\n",
    "best_k = best_trace[(:k,1)]\n",
    "println(\"Starting ks: $best_k\")\n",
    "traces = []\n",
    "push!(traces,best_trace)\n",
    "\n",
    "function nuts_parameters(trace)\n",
    "    \n",
    "    l = trace[:l]\n",
    "    param_selection = select()\n",
    "    for i=1:l+1 #Number of Layers\n",
    "        push!(param_selection, (:W,i))\n",
    "        push!(param_selection, (:b,i))\n",
    "    end\n",
    "    \n",
    "    prev_score = get_score(trace)\n",
    "    \n",
    "    acc = 0\n",
    "    for i=1:1\n",
    "        new_trace = NUTS(trace, param_selection, 0.65, m, m, false)[m+1]\n",
    "        new_score = get_score(new_trace)\n",
    "        if prev_score != new_score\n",
    "            println(\"Accepted\")\n",
    "            return (new_trace, 1)\n",
    "        else\n",
    "            return (trace, 0)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (trace, acc)\n",
    "end\n",
    "\n",
    "for i=1:iters\n",
    "    \n",
    "    trace_star = traces[i]\n",
    "    if i%10 == 0\n",
    "        println(\"Iteration $i: $(get_score(trace_star))\")\n",
    "        flush(stdout)\n",
    "    end\n",
    "    trace_star = gibbs_hyperparameters(trace_star)\n",
    "    trace_star, accepted = nuts_parameters(trace_star)\n",
    "\n",
    "    push!(traces,trace_star)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "#Run Inference\n",
    "#-------------\n",
    "\n",
    "Δ_max = 10\n",
    "acc_prob = 0.65\n",
    "#Random.seed!(1)\n",
    "\n",
    "scores = []\n",
    "traces = []\n",
    "ks = []\n",
    "across_acceptance = []\n",
    "within_acceptance = []\n",
    "\n",
    "#Inference Hyperparameters\n",
    "iters = 1\n",
    "m = 1\n",
    "\n",
    "@gen function gibbs_hyperparameters(trace)\n",
    "    obs_new = choicemap()::ChoiceMap\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    \n",
    "    for i=1:trace[:l] + 1\n",
    "        #Biases\n",
    "        bias = trace[(:b,i)]\n",
    "        \n",
    "        n = length(bias)\n",
    "        α = α₁ + (n/2)\n",
    "        \n",
    "        Σ = sum(bias.^2)/2 \n",
    "        β = 1/(1/β₁ + Σ)\n",
    "        \n",
    "        τᵦ ~ gamma(α,β)\n",
    "        \n",
    "        #Weights\n",
    "        β₂ = trace[(:k,1)]\n",
    "        \n",
    "        i == 1 ? α₀ = α₁ : α₀ = α₂\n",
    "        i == 1 ? β₀ = β₁ : β₀ = β₂\n",
    "        \n",
    "        weight = trace[(:W,i)]\n",
    "        \n",
    "        n = length(weight)\n",
    "        α = α₀ + (n/2)\n",
    "        \n",
    "        Σ = sum(weight.^2)/2\n",
    "        β = 1/(1/β₀ + Σ)\n",
    "        \n",
    "        τ ~ gamma(α,β)\n",
    "        \n",
    "        obs_new[(:τ,i)] = τ\n",
    "        obs_new[(:τᵦ,i)] = τᵦ\n",
    "    end\n",
    "    \n",
    "    (new_trace,_,_,_) = update(trace, args, argdiffs, obs_new)\n",
    "    \n",
    "    return new_trace\n",
    "end\n",
    "\n",
    "function nuts_parameters(trace)\n",
    "    \n",
    "    l = trace[:l]\n",
    "    param_selection = select()\n",
    "    for i=1:l+1 #Number of Layers\n",
    "        push!(param_selection, (:W,i))\n",
    "        push!(param_selection, (:b,i))\n",
    "    end\n",
    "    \n",
    "    prev_score = get_score(trace)\n",
    "    \n",
    "    acc = 0\n",
    "    for i=1:1\n",
    "        new_trace = NUTS(trace, param_selection, 0.65, m, m, false)[m+1]\n",
    "        new_score = get_score(new_trace)\n",
    "        if prev_score != new_score\n",
    "            #println(\"Accepted\")\n",
    "            return (new_trace, 1)\n",
    "        else\n",
    "            return (trace, 0)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (trace, acc)\n",
    "end\n",
    "\n",
    "function node_parameter(trace)\n",
    "    obs = obs_master\n",
    "    for i=1:trace[:l]+1\n",
    "        obs[(:τ,i)] = trace[(:τ,i)]\n",
    "        obs[(:τᵦ,i)] = trace[(:τᵦ,i)]\n",
    "    end\n",
    "    \n",
    "    init_trace = trace\n",
    "    \n",
    "    #################################################RJNUTS#################################################\n",
    "    #NUTS Step 1\n",
    "    trace_tilde = trace\n",
    "    for i=1:1\n",
    "        trace_tilde = gibbs_hyperparameters(trace_tilde)\n",
    "        (trace_tilde,) = nuts_parameters(trace_tilde)\n",
    "    end\n",
    "    \n",
    "    #Reversible Jump Step\n",
    "    (trace_prime, q_weight) = node_change(trace_tilde)\n",
    "    \n",
    "    #NUTS Step 2\n",
    "    trace_star = trace_prime\n",
    "    for i=1:1\n",
    "        (trace_star,) = nuts_parameters(trace_star)\n",
    "        trace_star = gibbs_hyperparameters(trace_star)\n",
    "    end\n",
    "    #################################################RJNUTS#################################################\n",
    "        \n",
    "    model_score = -get_score(init_trace) + get_score(trace_star)\n",
    "    across_score = model_score + q_weight\n",
    "\n",
    "    if rand() < exp(across_score)\n",
    "        println(\"********** Accepted: $(trace_star[(:k,1)]) **********\")\n",
    "        return (trace_star, 1)\n",
    "    else\n",
    "        return (init_trace, 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(y)\n",
    "    obs_master[(:y,i)] = y[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "\n",
    "(trace,) = generate(classifier, (xt,), obs)\n",
    "#trace = best_trace\n",
    "starting_k = trace[(:k,1)]\n",
    "println(\"Starting k: $starting_k\")\n",
    "traces = []\n",
    "\n",
    "for i=1:1000\n",
    "    (trace, accepted) = node_parameter(trace)\n",
    "    push!(across_acceptance, accepted)\n",
    "    trace  = gibbs_hyperparameters(trace)\n",
    "    (trace, accepted) = nuts_parameters(trace)\n",
    "    push!(within_acceptance, accepted)\n",
    "    push!(scores,get_score(trace))\n",
    "    push!(traces, trace)\n",
    "    println(\"$i : $(get_score(trace))\")\n",
    "    flush(stdout)\n",
    "    if i%10 == 0\n",
    "        a_acc = 100*(sum(across_acceptance)/length(across_acceptance))\n",
    "        w_acc = 100*(sum(within_acceptance)/length(within_acceptance))\n",
    "        println(\"Epoch $i A Acceptance Probability: $a_acc %\")\n",
    "        println(\"Epoch $i W Acceptance Probability: $w_acc %\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Posterior Score Chart\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(y)\n",
    "    obs_master[(:y,i)] = y[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "\n",
    "plot([get_score(trace) for trace in traces])\n",
    "plt.title(\"NUTS Score: MNIST Classification\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Posterior\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Training Accuracy\n",
    "accs = []\n",
    "dists = []\n",
    "for i = 1:length(traces)\n",
    "    trace = traces[i]\n",
    "    test_labels = G(xt,trace)\n",
    "    results = label_output(test_labels)\n",
    "    test_acc = sum([y[i] == results[i] for i=1:length(y)])/length(y)\n",
    "    test_dist = sum([euclidean(yt[:,j],test_labels[:,j]) for j=1:length(y)])\n",
    "    push!(accs,test_acc)\n",
    "    push!(dists,test_dist)\n",
    "end\n",
    "\n",
    "plot(accs)\n",
    "plt.title(\"NUTS Accuracy: MNIST Training Set Classification\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Testing Accuracy\n",
    "accs = []\n",
    "#dists = []\n",
    "for i = 1:length(traces)\n",
    "    trace = traces[i]\n",
    "    test_labels = G(xz,trace)\n",
    "    results = label_output(test_labels)\n",
    "    test_acc = sum([y_test[i] == results[i] for i=1:length(y_test)])/length(y_test)\n",
    "    test_dist = sum([euclidean(yzt[:,j],test_labels[:,j]) for j=1:length(y)])\n",
    "    push!(accs,test_acc)\n",
    "    #push!(dists,test_dist)\n",
    "end\n",
    "\n",
    "plot(accs)\n",
    "plt.title(\"NUTS Accuracy: MNIST Test Set Classification\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Posterior\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1]\n",
    "targets = []\n",
    "for i=1:length(traces)\n",
    "    trace = traces[i]\n",
    "    test_labels = G(xt,trace)\n",
    "    results = label_output(test_labels)\n",
    "    push!(targets,results[46])\n",
    "end\n",
    "hist(targets)\n",
    "println(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marginalize Probabilities\n",
    "j = 100\n",
    "targets = zeros(length(traces)-j,c,N)\n",
    "for i=j:length(traces)-j\n",
    "    trace = traces[i]\n",
    "    labels = G(xt,trace)\n",
    "    targets[i-j+1,:,:] = labels\n",
    "end\n",
    "\n",
    "sums = sum(targets,dims=1)[1,:,:]./(length(traces)-j)\n",
    "guesses = [findmax(sums[:,i])[2] for i=1:N]\n",
    "test_acc = sum([y[i] == guesses[i] for i=1:length(y)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
